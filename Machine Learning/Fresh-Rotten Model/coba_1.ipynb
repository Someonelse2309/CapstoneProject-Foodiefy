{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8609537a-3d27-4e43-880b-9646dc7e1722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import zipfile\n",
    "import random\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras import layers, models\n",
    "from shutil import copyfile\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab67af59-8f21-47e6-bd02-b975e1188e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_and_clean_images(folder_path):\n",
    "#     for folder_name in os.listdir(folder_path):\n",
    "#         folder_dir = os.path.join(folder_path, folder_name)\n",
    "#         for img_file in os.listdir(folder_dir):\n",
    "#             img_path = os.path.join(folder_dir, img_file)\n",
    "#             try:\n",
    "#                 # Muat gambar menggunakan PIL untuk memverifikasi keabsahannya\n",
    "#                 with Image.open(img_path) as img:\n",
    "#                     img.verify()\n",
    "#                 # Memuat gambar menggunakan TensorFlow untuk verifikasi tambahan\n",
    "#                 img_tensor = tf.io.read_file(img_path)\n",
    "#                 img_tensor = tf.image.decode_image(img_tensor)\n",
    "#             except (IOError, SyntaxError, tf.errors.InvalidArgumentError) as e:\n",
    "#                 print(f\"Corrupt or unsupported image removed: {img_path}\")\n",
    "#                 os.remove(img_path)\n",
    "\n",
    "# dataset_path = \"fresh-rotten/Train\"\n",
    "# check_and_clean_images(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8a2f65f-ad43-4eb1-a3cc-5c31d9b03d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (256, 256)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a088a25-14ba-437b-ac41-3f9ea8c86a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"fresh-rotten/Train\"\n",
    "val_path = \"fresh-rotten/Validation\"\n",
    "test_path = \"fresh-rotten/Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8fe390a-4363-414d-bdee-678dd4fedaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 33412 files belonging to 32 classes.\n",
      "Found 7127 files belonging to 32 classes.\n",
      "Found 7283 files belonging to 32 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = image_dataset_from_directory(\n",
    "    train_path,\n",
    "    label_mode=\"int\",         \n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "# .map(lambda x: (x, custom_label_mapping(x.file_path)))\n",
    "\n",
    "val_dataset = image_dataset_from_directory(\n",
    "    val_path,\n",
    "    label_mode=\"int\",\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "# .map(lambda x: (x, custom_label_mapping(x.file_path)))\n",
    "\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    test_path,\n",
    "    label_mode=\"int\",\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "# .map(lambda x: (x, custom_label_mapping(x.file_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "657fdc09-f071-4eca-9b75-8cb9864a738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_label_mapping(image, label):\n",
    "    new_label = tf.where(label < 9, 0, 1)\n",
    "    return image, new_label\n",
    "\n",
    "train_dataset = train_dataset.map(custom_label_mapping)\n",
    "val_dataset = val_dataset.map(custom_label_mapping)\n",
    "test_dataset = test_dataset.map(custom_label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "590761d8-a4dc-4d58-a70e-86e03bc687be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.1),\n",
    "    tf.keras.layers.RandomZoom(0.1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33799417-54b9-45ff-94d4-bbf6c2b6ee30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image, label):\n",
    "    # image = tf.image.resize(image, (256, 256))\n",
    "    image = tf.cast(image, tf.float32) / 255.0  \n",
    "    return image, label\n",
    "\n",
    "train_dataset = train_dataset.map(lambda x, y : (data_augmentation(x, training=True), y)).map(process_image)\n",
    "val_dataset = val_dataset.map(process_image)\n",
    "test_dataset = test_dataset.map(process_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5941dc4e-6dbb-4691-b1fa-a60827d68790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(256, 256, 3)),\n",
    "        layers.Rescaling(1/255.),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88218d1b-6195-4b10-b014-fc2fcb8d416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f02282cb-9b68-4f55-ae2e-a622132de927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1045/1045\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1392s\u001b[0m 1s/step - accuracy: 0.8363 - loss: 0.4072 - val_accuracy: 0.9122 - val_loss: 0.2212\n",
      "Epoch 2/10\n",
      "\u001b[1m1045/1045\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1334s\u001b[0m 1s/step - accuracy: 0.8938 - loss: 0.2395 - val_accuracy: 0.9277 - val_loss: 0.1718\n",
      "Epoch 3/10\n",
      "\u001b[1m1045/1045\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1874s\u001b[0m 2s/step - accuracy: 0.9123 - loss: 0.2034 - val_accuracy: 0.8941 - val_loss: 0.2977\n",
      "Epoch 4/10\n",
      "\u001b[1m1045/1045\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1351s\u001b[0m 1s/step - accuracy: 0.9119 - loss: 0.2118 - val_accuracy: 0.9454 - val_loss: 0.1285\n",
      "Epoch 5/10\n",
      "\u001b[1m1045/1045\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2474s\u001b[0m 2s/step - accuracy: 0.9285 - loss: 0.1655 - val_accuracy: 0.9341 - val_loss: 0.1587\n",
      "Epoch 6/10\n",
      "\u001b[1m1045/1045\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1349s\u001b[0m 1s/step - accuracy: 0.9388 - loss: 0.1502 - val_accuracy: 0.9530 - val_loss: 0.1187\n",
      "Epoch 7/10\n",
      "\u001b[1m1045/1045\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1330s\u001b[0m 1s/step - accuracy: 0.9461 - loss: 0.1295 - val_accuracy: 0.9503 - val_loss: 0.1320\n",
      "Epoch 8/10\n",
      "\u001b[1m1045/1045\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2017s\u001b[0m 2s/step - accuracy: 0.9555 - loss: 0.1135 - val_accuracy: 0.9620 - val_loss: 0.0994\n",
      "Epoch 9/10\n",
      "\u001b[1m1045/1045\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1535s\u001b[0m 1s/step - accuracy: 0.9584 - loss: 0.1069 - val_accuracy: 0.9710 - val_loss: 0.0794\n",
      "Epoch 10/10\n",
      "\u001b[1m1045/1045\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1391s\u001b[0m 1s/step - accuracy: 0.9648 - loss: 0.0920 - val_accuracy: 0.9660 - val_loss: 0.0839\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=10, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bf062ea-4881-4650-ad65-93dc6b62068f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 411ms/step - accuracy: 0.9674 - loss: 0.0815\n",
      "\u001b[1m1045/1045\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m494s\u001b[0m 472ms/step - accuracy: 0.9645 - loss: 0.0847\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 422ms/step - accuracy: 0.9496 - loss: 0.1198\n",
      "Train Accuracy\t\t: 96.72% | Train Loss\t\t: 0.08169645071029663\n",
      "Validation Accuracy\t: 96.60% | Validation Loss\t: 0.08388444781303406\n",
      "Test Accuracy\t\t: 94.92% | Test Loss \t\t: 0.12273943424224854\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model.evaluate(val_dataset)\n",
    "tra_loss, tra_acc = model.evaluate(train_dataset)\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print(f\"Train Accuracy\\t\\t: {tra_acc * 100:.2f}% | Train Loss\\t\\t: {tra_loss}\")\n",
    "print(f\"Validation Accuracy\\t: {val_acc * 100:.2f}% | Validation Loss\\t: {val_loss}\")\n",
    "print(f\"Test Accuracy\\t\\t: {test_acc * 100:.2f}% | Test Loss \\t\\t: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a2ab69d-b7c3-46db-a206-ed93756612c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"fresh-rotten.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1adc6c93-a9da-4695-85cb-9f5522235609",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"fresh-rotten.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f23f6dc-79b3-4b84-b28c-d6d4af6e6b94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
